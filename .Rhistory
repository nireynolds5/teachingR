mean(samples)
median(samples)
### LOSS FUNCTIONS ------------------------------------
# We can assume some kind of function for the cost of estimating the actual
# value of p incorrectly. For example, if we say the cost is proportional
# to the distance of our estimate d from p (i.e. abs(d-p)), then
# for a particular true p, we can calculate the cost of every decision d.
# If we now average this over all possible p but weighted by the posterior
# function, we get an overall loss function of d based on our information
# about p.
loss <- sapply(p_grid, function(d) sum(posterior*abs(d - p_grid)))
# The minimum of this function is actually the median of the posterior distribution.
p_grid[which.min(loss)]
points(p_grid, loss
, col = alpha("purple4", 0.1)
, pch = 19
, cex = 0.5
)
abline(v = p_grid[which.min(loss)]
, lwd = 3
, lty = 2
, col = "purple")
## VIDEO 2 (starting at about 1 h) AND CHAPTER 3 -----------------------
# Note that in representing uncertainty about a parameter estimate, we have
# two different sources or types of uncertainty:
# Randomness in choosing paths in the garden of forging data, i.e. sampling
# (maybe our sample is an odd case for this hypothesis/parameter value)
# VS
# Uncertainty about p (embodied by posterior)
### PREDICTIONS (Posterior predictive distributions) -----------------------
# For example, we might want to know how likely it is that we get
# 5 water out of 9 samples. What we know is the posterior distribution
# of how likely any proportion of water on the globe is.
# We can sample from the posterior distribution different proportions
# of water, then for each proportion of water calculate a predictive
# distribution for our new outcome; but then we add those across
# different samples from the posterior, to get an overall prediction
# that takes the uncertainty in the posterior into account.
# Recap: note that there are two different types of 'sampling' here:
no_of_samples # how often we toss the globe
samples_from_posterior <- samples # how many samples we draw from the posterior
# Now we're going to generate a posterior predictive distribution.
PPDist <- c()
for (i in 1:length(samples_from_posterior)) {
PPDist[i] <- sum(sim_globe(samples_from_posterior[i], no_of_samples) == "W")
}
# What is a PPD? It is a prediction of what outcomes we might get given a particular
# posterior. I.e. it simulates both types of uncertainty, the uncertainty about
# the true value of p (the parameter represented by the posterior) as well as
# the uncertainty given that even for a given p your experimental measurements
# (samples) might have different values.
# Here is an illustration of our PPD:
simplehist(PPDist
, xlab="Simulated water count"
, ylab="Frequency across samples from posterior"
)
# McElreath argues here that a creative model checker should come up with other
# ways to check than the obvious one - e.g. measuring other aspects of the PPDist than
# than which was used to generate the posterior.
# E.g. in this example, instead of counting the number of water, we could count the
# length of 'runs' of the same result, or the number of switches between W and L, or
# something else.
### Modeling more complex data collection processes -----------------
## What if there are errors in the data?
#  We can model these explicitly, and then use that additional information
#  to improve our inference and the estimate of our uncertainty.
# Start with actually modeling the generative process.
# p is the parameter we want to find out, here the proportion of water
# N is the number of samples we draw
# x1 is the false positive rate, i.e. getting water when it is land
# x2 is the false negative rate, i.e. getting land when it is water
sim_globe2 <- function(p, N, x1, x2) {
# The actual sample function is the same as before
truesample <- sample(c("W", "L"), size=N, prob = c(p, 1-p), replace = TRUE)
# Now we simulate what we observe, which is with a potential error
obs_sample <- ifelse(truesample=="W",
ifelse(runif(N)<x1, # this is 'random uniform' distribution of length N
"L", "W"), # if false negative, return L, otherwise W
# if the true sample was L to begin with
ifelse(runif(N)<x2,
"W", "L")
)
return(obs_sample)
}
## Grid approximation of our new model, i.e. calculating posterior knowing that
# there are error rates
grid_approx_w_error <- function(W, L, grid_size, x) {
# define grid of possible hypotheses/values of thing to be estimated
p_grid <- seq(from=0, to=1, length.out=grid_size)
# define prior across all hypotheses
prior <- rep(1, grid_size)
# Standardizing just for the sake of the graph
prior <- prior/sum(prior)
# Total samples
Z <- L+W
# THIS IS WHERE THE WORK OF THE MODEL HAPPENS:
# compute likelihood at each value in grid (i.e. data given each hypothesis)
likelihood <- ((((p_grid*(1-x)) + ((1-p_grid)*x))^W) * ((((1-p_grid)*(1-x)) + (p_grid*x))^L) )/ Z
# Formula in minute 1:31 in video 2
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
return(posterior)
}
# Ok let's do the whole thing. [The owl!]
# (1) simulate data!
# Assume a world to simulate:
prop_water_on_globe #<- 0.7
error_rate_fp <- 0.1
error_rate_fn <- 0.1
# Design experiment:
no_of_replicates <- 10 # of the experiment
no_of_samples <- 5 # measurements per experiment
# Simulate once to test it works
sim_globe2(prop_water_on_globe, no_of_samples, error_rate_fp, error_rate_fn)
# Replicate
(sim_replicates <- replicate(sim_globe2(prop_water_on_globe, no_of_samples, error_rate_fp, error_rate_fn), n=no_of_replicates))
test_results <- c()
for (i in 1:no_of_replicates) {
test_results[i] <- (sum(sim_replicates[,i] == "W"))
}
# Check that data produced look reasonable
simplehist(test_results
, xlab="Simulated water count"
, ylab="Frequency across replicates"
)
# (2) Test (fit model, i.e. calculate posterior) on simulated data
# Re-run everything as if these were our empirical results
total_no_of_samples <- no_of_replicates*no_of_samples
total_observations_of_water <- sum(sim_replicates == "W")
grid_size <- 100
W <- total_observations_of_water
L <- total_no_of_samples - total_observations_of_water
# Or don't do the simulated data but test out specific situations
#W <- 6
#L <- 3
# Now we're doing the actual model, i.e. posterior calculation - we'll do it
# twice, once with our new model above and once WITH THE MODEL AS IF THERE WAS NO ERROR
# As if there was no error (this one included a graph so that's what we get)
posterior <- grid_approx(W, L, grid_size)
# Our new model with error
posterior_with_error <- grid_approx_w_error(W, L, grid_size, error_rate_fn)
# And we need this for the graph
p_grid <- seq(from=0 , to=1 , length.out=grid_size)
transparency_of_points <- 15/grid_size + 0.05
y_max <- 1.1 * max(posterior, posterior_with_error)
# (3) Now we plot posterior with both models
plot(p_grid, posterior_with_error
, pch = 19
, type = "b"
, lwd = 2
, col = alpha("slateblue1", transparency_of_points)
, xlab="probability of water"
, ylab="posterior probability"
, ylim=c(0, y_max)
#, yaxt='n'
)
points(p_grid, posterior
, pch = 19
, col = alpha("red", transparency_of_points)
, type = "b"
, lwd = 2
)
prior <- rep(1, grid_size)
prior <- prior/sum(prior)
Z <- L+W
likelihood <- ((((p_grid*(1-error_rate_fn)) + ((1-p_grid)*error_rate_fn))^W) * ((((1-p_grid)*(1-error_rate_fn)) + (p_grid*error_rate_fn))^L) )/ Z
points(p_grid, prior
, pch = 19, col = "purple4"
, type = 'l'
, lwd = 3)
points(p_grid, likelihood * 0.02/max(likelihood)
, pch = 19, col = "darkseagreen"
, type = 'l'
, lwd = 3)
# As expected, since the prior is flat, the likelihood is very similar to the posterior
# with error (which is the likelihood I used). Also note that I rescaled the likelihood so
# its pattern would be visible.
w <- 6; n <- 9;
p_grid <- seq(from=0,to=1,length.out=100)
posterior <- dbinom(w,n,p_grid)*dunif(p_grid,0,1)
posterior <- posterior/sum(posterior)
plot(posterior)
MyDatafromGoogleSheets <-
MyDatafromGithub <- read.csv("https://raw.githubusercontent.com/shannonmcwaters/Directed-exploration/refs/heads/main/Maze%20Data%20Raw")
setwd("C:/Users/dornh/Dropbox/Github/teachingR")
# But note that this is a little problematic as this detailed path
# would never work on someone else's computer. Instead,
# you can also use
setwd("../teachingR")
# But note that this is a little problematic as this detailed path
# would never work on someone else's computer. Instead,
# you can also use
setwd("/teachingR")
# But note that this is a little problematic as this detailed path
# would never work on someone else's computer. Instead,
# you can also use
setwd("../teachingR")
# Or directly from a Google Sheet:
MyDatafromGoogleSheets <- read_csv("https://docs.google.com/spreadsheets/d/1K2D2rH770iDfZzlwakHK89bwvoPqWDLaHbVNsanJFX8/gviz/tq?tqx=out:csv")
# Or directly from a Google Sheet:
MyDatafromGoogleSheets <- read.csv("https://docs.google.com/spreadsheets/d/1K2D2rH770iDfZzlwakHK89bwvoPqWDLaHbVNsanJFX8/gviz/tq?tqx=out:csv")
library(rstan)
#install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
## Then install 'rethinking' package for the book-specific functions.
# From 2019 book:
#install.packages(c("coda","mvtnorm","devtools"))
library(devtools)
#devtools::install_github("rmcelreath/rethinking",ref="Experimental")
#devtools::install_github("rmcelreath/rethinking", force=TRUE)
# From R Documentation:
#devtools::install_github("stan-dev/cmdstanr")
#install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))
#devtools::install_github("rmcelreath/rethinking")
## LIBRARIES  -----------------------------
# Do I need these? I don't think so
#library(cmdstanr)
#library(posterior)
#library(bayesplot)
#color_scheme_set("brightblue")
library(rethinking)
library(scales)
no_of_samples <- 9
no_of_successes <- 6
# Here this is the probability density for a binomial function
# which is what you need when drawing with replacement
dbinom(no_of_successes, size=no_of_samples , prob=0.5 )
# Here this is the probability density for a binomial function
# which is what you need when drawing with replacement
pbinom(no_of_successes, size=no_of_samples, prob=0.5)
# Here this is the probability density for a binomial function
# which is what you need when drawing with replacement
dbinom(no_of_successes, size=no_of_samples, prob=0.5)
# Here this is the probability density for a binomial function
# which is what you need when drawing with replacement
dbinom(2, size=no_of_samples, prob=0.5)
# Here this is the probability density for a binomial function
# which is what you need when drawing with replacement
dbinom(0, size=no_of_samples, prob=0.5)
library(rethinking)
library(scales)  # for number_format & color scales
library(viridis)
# Graphics setup -----------------------------
no_categories <- 6
## Colorpalette
twogroupcolors <- c("#5a9a8f", "#7b6ea8")
threegroupcolors <- viridis(3)
severalcategories_colors <- magma(no_categories)
## Similar to colors, it often makes sense to define some other things
## universally for all your plots, e.g. margins, where the sample sizes
## are plotted, y-axis range. This depends on your figures though.
y_max <- 6
y_min <- 0
N_y_offset <- 0.95 # This puts sample size numbers 5% below max, for example
# IMPORT ----------------------------------
# A common problem when reading any files is that you must make sure
# that R uses the folder you want as 'working directory'. You can
# pick the working directory from the 'Session' menu above; or
# you can write the path here in the code, e.g.:
setwd("C:/Users/dornh/Dropbox/Github/teachingR")
# But note that this is a little problematic as this detailed path
# would never work on someone else's computer. Instead,
# you can also use
setwd("../teachingR")
# or similar ... the ".." means 'go up one directory level' and then
# it will go into the folder to the right of the "/".
MyData <- read.table("bb col data.csv"
, header=T
, row.names=1
, sep = ","
, dec = "."
)
# Or you can load a .csv file from Google Drive:
path <- "C:/Users/dornh/Dropbox/TEACHING/ECOL496G 596G Spring2025 Networks/network datasets/Memmott pollinator network/"
MyDatafromGoogleDrive <- read.table(paste(path, "memmott_1999.csv", sep = ""), header = TRUE, sep = ",")
# Or directly from a Google Sheet:
MyDatafromGoogleSheets <- read.csv("https://docs.google.com/spreadsheets/d/1K2D2rH770iDfZzlwakHK89bwvoPqWDLaHbVNsanJFX8/gviz/tq?tqx=out:csv")
# Or directly from github:
MyDatafromGithub <- read.csv("https://raw.githubusercontent.com/shannonmcwaters/Directed-exploration/refs/heads/main/Maze%20Data%20Raw")
View(MyDatafromGoogleDrive)
View(MyDatafromGoogleSheets)
# Bare boxplot
boxplot(avgsize ~ treatment
, data = MyData
, xlab = "X Concept [unit measured]"
, ylab = "Y Concept [unit measured]"
, col = threegroupcolors
, range = 0
)
# GRAPHING THINGS ------------------------------
# Bare boxplot
boxplot(avgsize ~ treatment
, data = MyData
, xlab = "X Concept [unit measured]"
, ylab = "Y Concept [unit measured]"
, col = threegroupcolors
, range = 0
)
# Fancy boxplot - but this should be your default
# What makes it fancy:
# Adjust margins
# Add sample sizes
# Add data points
# Margins:
par(oma = c(2,2,2,2), mar = c(4,4,1,1), mgp=c(3, 1, 0), las=1)
# bottom, left, top, right
par(mfrow=c(1,1))
# How to make a great boxplot -
# Saving the plot into a variable allows us to access plot parameters afterwards.
Nice_Plot <- boxplot(avgsize ~ treatment
, data = MyData
, xlab = "X Concept [unit measured]"
, ylab = "Y Concept [unit measured]"
, range = 0
# Always make axis descriptions as clear and comprehensive as possible
, names = c("Large bees", "Middling bees", "Small bees")
, col = alpha(threegroupcolors, 0.5) # use same colors as elsewhere,
# but slightly transparent so we can see data points
, ylim = c(y_min, y_max) # always think about the scale - starting from zero is typically better
)
# Putting sample sizes above bars
nbGroup <- nlevels(as.factor(Nice_Plot$names)) # this is just a way to extract
# category names from the plot - you could get this directly from data
text(x=c(1:nbGroup)
, y=N_y_offset*y_max
, cex = 1
, col = threegroupcolors
, paste("N=", Nice_Plot$n, sep="")  # again, the sample size 'n' is directly extracted from the plot
)
mtext("additional margin label", side=2, line=2, las=0)
mtext("additional margin label on outside", side=1, line=5, las=0, xpd = TRUE)
# Represent the raw data as well, especially for mid- to low sample sizes.
stripchart(avgsize ~ treatment
, data = MyData
, add = TRUE # this plots this graph on top of the existing one
, pch = 19
, col = threegroupcolors
, method = "jitter"
, jitter = 0.2
, vertical = TRUE
)
View(MyDatafromGoogleDrive)
View(MyData)
graph_data <- MyData
# We can even upgrade this to a multi-panel plot. For this we can use another par()
# setting (e.g. mfrow=c(2,2)), or for more control use 'layout()'.
layout(matrix(c(1,2,0,3), 2, 2, byrow = T), widths=c(1,5),
heights=c(5,1))
# This gives us a four-panel plot; the plots will be inserted into the panels in
# order by row.
# Various other format adjustments
par(oma = c(0,0,0,0), mgp=c(3, 1, 0), las=1)
# Panel 1:
par(mar = c(4,0,1,0)) # bottom, left, top, right
boxplot(table(avgsize)
, data = graph_data
, xaxt = 'n'
, yaxt = 'n'
, frame = FALSE
, range = 0
, col = color_distributions
)
boxplot(table(graph_data$avgsize)
, data = graph_data
, xaxt = 'n'
, yaxt = 'n'
, frame = FALSE
, range = 0
, col = color_distributions
)
boxplot(table(graph_data$avgsize)
, data = graph_data
, xaxt = 'n'
, yaxt = 'n'
, frame = FALSE
, range = 0
#        , col = color_distributions
)
table(graph_data$avgsize)
# But note that this is a little problematic as this detailed path
# would never work on someone else's computer. Instead,
# you can also use
setwd("../teachingR/example_data")
# But note that this is a little problematic as this detailed path
# would never work on someone else's computer. Instead,
# you can also use
setwd("../teachingR")
# A common problem when reading any files is that you must make sure
# that R uses the folder you want as 'working directory'. You can
# pick the working directory from the 'Session' menu above; or
# you can write the path here in the code, e.g.:
setwd("C:/Users/dornh/Dropbox/Github/teachingR")
# Import a whole list of files,
# specifically all csv files from folder '
files <- (Sys.glob("example_data/*.csv"))
files <- (Sys.glob("example_data/*.csv"))
# Initiate a blank data frame
EnormousData <- data.frame()
# Read content of all files into a list
listOfDataframes <- lapply(files,
function(x) {
read.table(x,
header = T,
sep = ",",
skip = 6
)
}
)
# Add all the rows from all the files together
EnormousData <- do.call("bind_rows", listOfDataframes)
# Or you can load a .csv file from Google Drive:
path <- "https://drive.google.com/drive/folders/1aZWvhJjgTH9QTrD9hV1LiPiOKFRxmH7x/"
MyDatafromGoogleDrive <- read.table(paste(path, "memmott_1999.csv", sep = ""), header = TRUE, sep = ",")
path <- "https://drive.google.com/drive/folders/1aZWvhJjgTH9QTrD9hV1LiPiOKFRxmH7x/"
files <- (Sys.glob(paste(path, "*.xlsx")))
EnormousData <- data.frame()
# Read content of all files into a list
listOfDataframes <- lapply(files,
function(x) {
read.table(x,
header = T,
sep = ",",
)
}
)
# Add all the rows from all the files together
EnormousData <- do.call("bind_rows", listOfDataframes)
library(tidyverse)
# Add all the rows from all the files together
EnormousData <- do.call("bind_rows", listOfDataframes)
# Or for local files:
path <- "/example_data/similardatasheets"
files <- (Sys.glob(paste(path, "*.csv")))
# Or for local files:
path <- "/example_data/similardatasheets/"
pathwild <- paste(path, "*.csv")
files <- (Sys.glob(pathwild))
files <- (Sys.glob("/example_data/similardatasheets/*.csv"))
files <- (Sys.glob("./example_data/similardatasheets/*.csv"))
files
path <- "./example_data/similardatasheets/"
pathwild <- paste(path, "*.csv")
files <- (Sys.glob(pathwild))
EnormousData <- data.frame()
# Read content of all files into a list
listOfDataframes <- lapply(files,
function(x) {
read.table(x,
header = T,
sep = ",",
skip = 6
)
}
)
# Add all the rows from all the files together
EnormousData <- do.call("bind_rows", listOfDataframes)
read.table(files[[1]])
str(files)
# Or for local files:
files <- (Sys.glob("./example_data/similardatasheets/*.csv"))
EnormousData <- data.frame()
# Read content of all files into a list
listOfDataframes <- lapply(files,
function(x) {
read.table(x,
header = T,
sep = ",",
skip = 6
)
}
)
# Add all the rows from all the files together
EnormousData <- do.call("bind_rows", listOfDataframes)
#
install.packages("googledrive")
library(googledrive)
drive_ls(path = "https://drive.google.com/drive/folders/1aZWvhJjgTH9QTrD9hV1LiPiOKFRxmH7x/")
drive_ls(path = "https://drive.google.com/drive/folders/1xIhjFMzv9D3p2s88lG-5wHzu_aXjmLWx")
library(googledrive)
drive_ls(path = "https://drive.google.com/drive/folders/1xIhjFMzv9D3p2s88lG-5wHzu_aXjmLWx")
install.packages("readxl")
library(readxl)
library(tidyverse) # for bind.rows
# File access
library(googledrive)
library(readxl)
# Colors
library(scales)  # for number_format & color scales
library(viridis)
drive_ls(path = "~/")
# File access
library(googledrive)
drive_ls(path = "~/")
library(tidyverse) # for bind.rows
# File access
library(googledrive)
library(readxl)
# Colors
library(scales)  # for number_format & color scales
library(viridis)
# Making output tables for statistical models
library(sjPlot) # for linear models output tables
drive_list()
# File access
library(googledrive)
drive_list()
installed.packages()
detach("package:ggplot2", unload = TRUE)
library(ggplot2)
drive_ls()
drive_ls()
